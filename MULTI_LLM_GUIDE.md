# Dream Reader - Multi-LLM Chat Interface

Una interfaz de chat moderna que soporta m√∫ltiples proveedores de LLM con APIs compatibles con el est√°ndar OpenAI.

## Proveedores Soportados

### ü§ñ OpenAI
- **Modelos**: GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, GPT-4o, GPT-4o Mini
- **API Key**: Obt√©n tu clave desde [OpenAI Platform](https://platform.openai.com/api-keys)
- **Formato**: `sk-...`

### üß† Anthropic (Claude)
- **Modelos**: Claude 3 Sonnet, Claude 3 Opus, Claude 3 Haiku, Claude 3.5 Sonnet
- **API Key**: Obt√©n tu clave desde [Anthropic Console](https://console.anthropic.com/)
- **Formato**: `sk-ant-...`

### ‚ö° Groq
- **Modelos**: Mixtral 8x7B, Llama2 70B, Gemma 7B, Llama3 8B/70B
- **API Key**: Obt√©n tu clave desde [Groq Console](https://console.groq.com/keys)
- **Formato**: `gsk_...`
- **Ventaja**: Inferencia ultra-r√°pida

### üåê Together AI
- **Modelos**: Meta Llama 2, Mixtral, Nous Hermes
- **API Key**: Obt√©n tu clave desde [Together AI](https://api.together.xyz/settings/api-keys)
- **Ventaja**: Modelos open source

### üè† Ollama (Local)
- **Modelos**: Llama2, CodeLlama, Mistral, Mixtral
- **Setup**: Instala [Ollama](https://ollama.ai/) y ejecuta `ollama serve`
- **Ventaja**: Ejecuta modelos localmente, sin API key requerida

### üîç Perplexity AI
- **Modelos**: Llama 3.1 Sonar (Small/Large/Huge)
- **API Key**: Obt√©n tu clave desde [Perplexity Settings](https://www.perplexity.ai/settings/api)
- **Formato**: `pplx-...`
- **Ventaja**: Modelos con capacidades de b√∫squeda

### üéØ Google Gemini
- **Modelos**: Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 1.0 Pro
- **API Key**: Obt√©n tu clave desde [Google AI Studio](https://aistudio.google.com/app/apikey)
- **Formato**: `AIza...`
- **Ventaja**: Modelos multimodales de Google

## Caracter√≠sticas

- ‚úÖ **Interfaz unificada** para todos los proveedores
- ‚úÖ **Configuraci√≥n per-proveedor** con temperatura y max tokens ajustables
- ‚úÖ **URLs base personalizables** para proxies o deployments privados
- ‚úÖ **Almacenamiento local** de configuraciones (no se env√≠an a nuestros servidores)
- ‚úÖ **Migraci√≥n autom√°tica** desde configuraciones anteriores
- ‚úÖ **Manejo de errores** espec√≠fico por proveedor
- ‚úÖ **Detecci√≥n autom√°tica** de problemas de conectividad
- ‚úÖ **Compatibilidad con Ollama** para uso local

## C√≥mo usar

1. **Configurar Proveedor**: Haz clic en el √≠cono ‚ö° para abrir la configuraci√≥n
2. **Seleccionar Proveedor**: Elige tu proveedor preferido de LLM
3. **Agregar API Key**: Ingresa tu clave API (si es requerida)
4. **Seleccionar Modelo**: Elige el modelo espec√≠fico que deseas usar
5. **Ajustar Configuraci√≥n**: Opcional - ajusta temperatura y max tokens
6. **¬°Chatear!**: Comienza a conversar con tu modelo seleccionado

## URLs Base Personalizadas

Puedes usar URLs base personalizadas para:
- Proxies OpenAI-compatibles
- Deployments privados
- Servicios auto-hospedados
- Gateways de API personalizados

## Ejemplos de Configuraci√≥n

### Para usar Ollama localmente:
```
Proveedor: Ollama (Local)
Modelo: llama2
URL Base: http://localhost:11434/v1
API Key: No requerida
```

### Para usar un proxy OpenAI:
```
Proveedor: OpenAI
Modelo: gpt-3.5-turbo
URL Base: https://tu-proxy.com/v1
API Key: tu-api-key
```

## Notas de Seguridad

- üîí Las API keys se almacenan solo en tu navegador
- üîí Nunca enviamos tus claves a nuestros servidores
- üîí Las requests van directamente a tu proveedor seleccionado
- üîí Puedes usar el modo local con Ollama para m√°xima privacidad

## Soluci√≥n de Problemas

### Error de conexi√≥n con Ollama
Aseg√∫rate de que Ollama est√© ejecut√°ndose:
```bash
ollama serve
```

### Error 401 - API Key inv√°lida
Verifica que tu API key sea correcta y tenga los permisos necesarios.

### Error 429 - Rate limit
Algunos proveedores tienen l√≠mites de rate. Espera un momento antes de reintentar.

### Error 503 - Servicio no disponible
El proveedor puede estar experimentando problemas. Intenta con otro proveedor.
